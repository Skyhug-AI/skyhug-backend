from dataclasses import dataclass
import json
import asyncio
from datetime import datetime, timezone

from openai import OpenAI
from realtime import RealtimeSubscribeStates

from supabase import Client
from supabase._async.client import AsyncClient
from repositories.messages import MessageRepository
from repositories.conversations import ConversationRepository
from repositories.therapists import TherapistRepository

from constants.prompts import (
    DEFAULT_SYSTEM_PROMPT,
    SKY_EXAMPLE_DIALOG,
    PERSONA_TEMPLATE,
    FUNCTION_DEFS,
)

@dataclass
class ChatService:
    supabase_sync: Client
    supabase_async: AsyncClient
    openai_client: OpenAI
    message_repo: MessageRepository
    conversation_repo: ConversationRepository
    therapist_repo: TherapistRepository
    START_TS: str = datetime.now(timezone.utc).isoformat()
    MAX_HISTORY: int = 10

    def build_chat_payload(self, conv_id: str, voice_mode: bool = False) -> list[dict]:
        """
        1) Load memory_summary (and clear â€œneeds_resummarizationâ€ if flagged)
        2) Load message history
        3) Load any therapist override (system_prompt)
        4) Build `system_prompt` (override > persona_template > default)
        5) Prepend that + SKY_EXAMPLE_DIALOG
        6) Append â€œmemoryâ€ message if brandâ€new conversation
        7) Turn DB rows into chat turns
        8) If too many turns, ask OpenAI for a brief summary of older turns
        """
        # Fetch any saved memory
        memory = self.conversation_repo.fetch_memory_summary(conv_id)

        # 1a) if flagged for resummarization, clear it
        self.conversation_repo.clear_memory_if_resummarize_flag(conv_id)
        memory = "" if self.conversation_repo.fetch_memory_summary(conv_id) == "" else self.conversation_repo.fetch_memory_summary(conv_id)

        # Fetch the message history
        history = self.message_repo.fetch_history_for_conversation(conv_id)

        # fetch which therapist this convo is using
        therapist_id = self.conversation_repo.fetch_therapist_id(conv_id)

        if therapist_id:
            trow = self.therapist_repo.fetch_therapist_persona(therapist_id)
        else:
            trow = {}

        # 4) pick which system_prompt to use
        if trow.get("system_prompt"):
            system_prompt = trow["system_prompt"]
        elif trow:
            specialties_list = ", ".join(trow.get("specialties", []))
            system_prompt = PERSONA_TEMPLATE.format(
                name=trow["name"],
                description=trow["description"],
                bio=trow["bio"],
                approach=trow["approach"],
                session_structure=trow["session_structure"],
                specialties_list=specialties_list,
            )
        else:
            system_prompt = DEFAULT_SYSTEM_PROMPT

        # 5) build initial messages
        messages: list[dict] = [{"role": "system", "content": system_prompt}] + SKY_EXAMPLE_DIALOG

        # 5a) if brandâ€new but memory exists, inject a â€œlast time we talked aboutâ€¦â€ message
        if memory and not history:
            messages.append({
                "role": "assistant",
                "content": f"Last time we spoke, we discussed {memory}. Would you like to continue?"
            })

        # 6) convert DB rows into chat turns
        turns: list[dict] = []
        for m in history:
            if m["sender_role"] == "user":
                turns.append({"role": "user", "content": m["transcription"]})
            else:
                turns.append({"role": "assistant", "content": m["assistant_text"]})

        # 7) if too many turns, ask GPT to summarize the older ones
        if len(turns) > self.MAX_HISTORY:
            summary_resp = self.openai_client.chat.completions.create(
                model="gpt-4-turbo" if voice_mode else "gpt-4-turbo",
                messages=messages
                         + [{"role": "assistant", "content": "Please summarize the earlier conversation briefly."}]
                         + turns[:-self.MAX_HISTORY],
                temperature=0.3,
                max_tokens=600,
            )
            summary = summary_resp.choices[0].message.content
            messages += [
                {"role": "assistant", "content": f"Summary of earlier conversation: {summary}"}
            ] + turns[-self.MAX_HISTORY:]
        else:
            messages += turns

        return messages

    def handle_ai_record(self, msg: dict) -> None:
        """
        1) Mark msg.ai_started = True
        2) Check voice_enabled on the conversation
        3) Build chat payload
        4) Pick model based on user_text
        5) If chat mode: stream deltas into DB
           If voice mode: run full completion, insert assistant_text + snippet_url
        6) Finally set original msg.ai_status = "done"
        """
        # 1) skip if already started
        if msg.get("ai_started"):
            return

        self.supabase_sync.table("messages") \
            .update({"ai_started": True}) \
            .eq("id", msg["id"]) \
            .execute()

        print(f"ðŸ’¬ â³ Generating AI reply for message {msg['id']}â€¦")
        try:
            # 2) figure out if voice_mode is on
            conv_row = (
                self.supabase_sync
                .table("conversations")
                .select("voice_enabled")
                .eq("id", msg["conversation_id"])
                .single()
                .execute()
            )
            voice_mode = bool(conv_row.data.get("voice_enabled", False))

            # 3) build payload
            payload = self.build_chat_payload(msg["conversation_id"], voice_mode=voice_mode)

            # 4) model selection
            user_text = (msg.get("transcription") or "").strip()
            lc = user_text.lower()

            if lc.startswith(("what is ", "define ")):
                model_name, max_tokens = "gpt-3.5-turbo", 150
            elif lc.startswith(("i feel", "iâ€™m feeling", "i am feeling", "i am", "i'm")):
                model_name, max_tokens = "gpt-4-turbo", 600
            elif lc.startswith(("why ", "how ", "explain ", "describe ", "compare ", "recommend ", "suggest ")):
                model_name, max_tokens = "gpt-4-turbo", 600
            else:
                words = [w for w in user_text.split() if w.strip()]
                if len(words) > 6:
                    model_name, max_tokens = "gpt-4-turbo", 600
                else:
                    model_name, max_tokens = "gpt-3.5-turbo", 150

            print("Selected model:", model_name)

            # 5) generate & store assistant reply
            if not voice_mode:
                # â€”â€” CHAT MODE: stream deltas into a new â€œassistantâ€ row â€”â€”
                insert_resp = self.supabase_sync.table("messages").insert({
                    "conversation_id": msg["conversation_id"],
                    "sender_role":     "assistant",
                    "assistant_text":  "",
                    "ai_status":       "pending",
                    "ai_started":      False,
                    "tts_status":      "done"
                }).execute()
                mid = insert_resp.data[0]["id"]

                # stream GPTâ€style responses back into that â€œassistant_textâ€ column
                stream = self.openai_client.chat.completions.create(
                    model=model_name,
                    messages=payload,
                    temperature=0.7,
                    stream=True,
                    max_tokens=max_tokens
                )

                accumulated = ""
                finish_reason = None
                for chunk in stream:
                    delta = chunk.choices[0].delta.content or ""
                    accumulated += delta
                    if chunk.choices[0].finish_reason:
                        finish_reason = chunk.choices[0].finish_reason

                    # write partial text back
                    self.supabase_sync.table("messages") \
                        .update({"assistant_text": accumulated}) \
                        .eq("id", mid) \
                        .execute()

                # if truncated midâ€sentence, send a continuation prompt
                if finish_reason == "length" or not accumulated.strip().endswith((".", "!", "?")):
                    cont = self.openai_client.chat.completions.create(
                        model=model_name,
                        messages=payload + [{"role": "assistant", "content": accumulated}],
                        temperature=0.7,
                        max_tokens=200
                    )
                    extra = cont.choices[0].message.content or ""
                    accumulated = accumulated.rstrip() + " " + extra.strip()
                    self.supabase_sync.table("messages") \
                        .update({"assistant_text": accumulated}) \
                        .eq("id", mid) \
                        .execute()

                # mark AI done
                self.supabase_sync.table("messages") \
                    .update({"ai_status": "done"}) \
                    .eq("id", mid) \
                    .execute()

            else:
                # â€”â€” VOICE MODE: full completion + snippet_url â€”â€”
                resp = self.openai_client.chat.completions.create(
                    model=model_name,
                    messages=payload,
                    temperature=0.7,
                    max_tokens=max_tokens,
                    functions=FUNCTION_DEFS,
                    function_call="auto"
                )
                choice = resp.choices[0].message

                # handle function calls (e.g. suicidal mentions)
                if getattr(choice, "function_call", None):
                    args = json.loads(choice.function_call.arguments)
                    content = (
                        "I'm so sorry youâ€™re feeling this way. "
                        f"If you ever think about harming yourself, call {args['hotline_number']}."
                    )
                else:
                    # base content
                    content = choice.content or ""
                    finish_reason = resp.choices[0].finish_reason
                    if finish_reason == "length" or not content.strip().endswith((".", "!", "?")):
                        cont = self.openai_client.chat.completions.create(
                            model=model_name,
                            messages=payload + [{"role": "assistant", "content": content}],
                            temperature=0.7,
                            max_tokens=200,
                            functions=FUNCTION_DEFS,
                            function_call="auto"
                        )
                        extra = cont.choices[0].message.content or ""
                        content = content.rstrip() + " " + extra.lstrip()

                # insert the row with full assistant_text
                insert_resp = self.supabase_sync.table("messages").insert({
                    "conversation_id": msg["conversation_id"],
                    "sender_role":     "assistant",
                    "assistant_text":  content,
                    "ai_status":       "done",
                    "tts_status":      "pending",
                    "snippet_url":     ""
                }).execute()
                mid = insert_resp.data[0]["id"]

                # seed snippet_url
                snippet_url = f"/tts-stream/{mid}?snippet=0"
                try:
                    self.supabase_sync.table("messages") \
                        .update({"snippet_url": snippet_url}) \
                        .eq("id", mid) \
                        .execute()
                except Exception:
                    self.supabase_sync.table("messages") \
                        .update({"tts_status": "error"}) \
                        .eq("id", mid) \
                        .execute()

            # 6) mark the original user message AIâ€done
            self.supabase_sync.table("messages") \
                .update({"ai_status": "done"}) \
                .eq("id", msg["id"]) \
                .execute()

            print(f"âœ… Assistant response created for message {msg['id']}")
        except Exception as e:
            print(f"âŒ AI error for {msg['id']}: {e}")
            self.supabase_sync.table("messages") \
                .update({"ai_status": "error"}) \
                .eq("id", msg["id"]) \
                .execute()

    async def start_realtime(self) -> None:
        """
        Kick off a Realtime subscription to â€œmessagesâ€ table. Whenever
        a new userâ€message row arrives (or gets edited), call handle_ai_record.
        """

        def on_insert(payload):
            msg = payload["data"]["record"]
            # only pick up new text messages (or audio â†’ transcription complete)
            if (
                msg["sender_role"] == "user"
                and msg.get("ai_status") == "pending"
                and msg.get("transcription_status") == "done"
                and not msg.get("ai_started")
            ):
                loop = asyncio.get_event_loop()
                loop.run_in_executor(None, self.handle_ai_record, msg)

        def on_update(payload):
            msg = payload["data"]["record"]
            # only pick up true edits (trascription â†’ done, or userâ€edited)
            if (
                msg["sender_role"] == "user"
                and msg.get("ai_status") == "pending"
                and msg.get("edited_at")  # only set by your editâ€message call
                and not msg.get("ai_started")
            ):
                loop = asyncio.get_event_loop()
                loop.run_in_executor(None, self.handle_ai_record, msg)

        def on_subscribe(status, err):
            if status == RealtimeSubscribeStates.SUBSCRIBED:
                print("ðŸ”Œ SUBSCRIBED to messages_changes")
            else:
                print("â— Realtime status:", status, err)

        channel = self.supabase_async.channel("messages_changes")
        channel.on_postgres_changes(event="INSERT", schema="public", table="messages", callback=on_insert)
        channel.on_postgres_changes(event="UPDATE", schema="public", table="messages", callback=on_update)
        await channel.subscribe(on_subscribe)

        # never return
        await asyncio.Event().wait()

    def fetch_pending(self, table: str, **conds) -> list[dict]:
        """
        Fetch rows matching conds from the given table. If table == "messages",
        only return any that were created after self.START_TS.
        """
        q = self.supabase_sync.table(table).select("*").match(conds)
        if table == "messages":
            q = q.gt("created_at", self.START_TS)
        return q.execute().data or []


    # def schedule_cleanup(self, interval_hours: int = 1) -> None:
    #     """
    #     Run close_inactive_conversations() every interval_hours via a repeating Timer.
    #     """
    #     from app.services.summarizer_service import SummarizerService

    #     def job():
    #         summarizer = SummarizerService(
    #             message_repo=self.message_repo,
    #             conversation_repo=self.conversation_repo,
    #             openai_service=<Injected OpenAIService>  # however you wire it
    #         )
    #         summarizer.close_inactive_conversations(interval_hours=interval_hours)
    #         threading.Timer(interval_hours * 3600, job).start()

    #     job()
